{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 | Bronze Ingestion\n",
        "\n",
        "## Stage Contract\n",
        "- Download CSVs from FiveThirtyEight with retries.\n",
        "- Standardize column names to snake_case.\n",
        "- Add Bronze metadata columns.\n",
        "- Write deterministic Parquet outputs to `lakehouse/bronze/<dataset>/data.parquet`."
      ],
      "id": "51c24a79"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Parameters\n",
        "source = \"fivethirtyeight\"\n",
        "dataset = \"recent_grads,bechdel_movies\"\n",
        "run_date = \"2026-02-22\"\n",
        "force_refresh = False"
      ],
      "id": "6ef9b483"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT_DIR = Path.cwd()\n",
        "if str(ROOT_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT_DIR))"
      ],
      "id": "5e9277e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from src.common.datasets import DATASETS, parse_dataset_argument\n",
        "from src.common.io import update_pipeline_metrics, update_stage_metrics\n",
        "from src.common.paths import BRONZE_DIR, RAW_DATA_DIR\n",
        "from src.common.pipeline import (\n",
        "    add_bronze_metadata,\n",
        "    download_csv_with_retries,\n",
        "    standardize_column_names,\n",
        "    write_parquet,\n",
        ")\n",
        "\n",
        "force_refresh_flag = str(force_refresh).lower() in {'1', 'true', 'yes'}\n",
        "selected_datasets = parse_dataset_argument(dataset)\n",
        "batch_id = f\"{run_date.replace('-', '')}_{source}\"\n",
        "\n",
        "stage_start = time.perf_counter()\n",
        "rows_ingested = 0\n",
        "rows_bronze = 0\n",
        "summary_rows = []\n",
        "\n",
        "for dataset_name in selected_datasets:\n",
        "    config = DATASETS[dataset_name]\n",
        "    raw_path = RAW_DATA_DIR / config['raw_filename']\n",
        "\n",
        "    download_csv_with_retries(\n",
        "        config['url'],\n",
        "        raw_path,\n",
        "        force_refresh=force_refresh_flag,\n",
        "        max_attempts=3,\n",
        "    )\n",
        "\n",
        "    raw_df = pd.read_csv(raw_path)\n",
        "    rows_ingested += len(raw_df)\n",
        "\n",
        "    standardized_df = standardize_column_names(raw_df)\n",
        "    bronze_df = add_bronze_metadata(\n",
        "        standardized_df,\n",
        "        source=source,\n",
        "        dataset=dataset_name,\n",
        "        file_path=raw_path,\n",
        "        batch_id=batch_id,\n",
        "    )\n",
        "\n",
        "    bronze_path = BRONZE_DIR / dataset_name / 'data.parquet'\n",
        "    write_parquet(bronze_df, bronze_path)\n",
        "\n",
        "    rows_bronze += len(bronze_df)\n",
        "    summary_rows.append(\n",
        "        {\n",
        "            'dataset': dataset_name,\n",
        "            'rows_raw': len(raw_df),\n",
        "            'rows_bronze': len(bronze_df),\n",
        "            'raw_path': str(raw_path),\n",
        "            'bronze_path': str(bronze_path),\n",
        "        }\n",
        "    )\n",
        "\n",
        "runtime_seconds = round(time.perf_counter() - stage_start, 2)\n",
        "\n",
        "update_stage_metrics(\n",
        "    'bronze',\n",
        "    {\n",
        "        'runtime_seconds': runtime_seconds,\n",
        "        'rows_ingested': rows_ingested,\n",
        "        'rows_bronze': rows_bronze,\n",
        "        'datasets_processed': selected_datasets,\n",
        "        'batch_id': batch_id,\n",
        "    },\n",
        ")\n",
        "update_pipeline_metrics({'rows_ingested': rows_ingested, 'rows_bronze': rows_bronze})\n",
        "\n",
        "pd.DataFrame(summary_rows)"
      ],
      "id": "d9a13f19"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}